import os
import openai
import numpy as np
import time
from tqdm import tqdm

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.embeddings.openai import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI

from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from IPython.display import HTML, display
openai.api_key = os.getenv("OPENAI_API_KEY")


def get_file_path(folder_path='/Users/kremerr/Documents/GitHub/RARR/trivia_qa/evidence/wikipedia'):

    file_paths = []
    # Walk through the directory
    for root, dirs, files in os.walk(folder_path):
        for file in tqdm(files, desc="Saving file paths...", leave=False):
            if file.endswith('.txt'):
                # Construct the full file path and add it to the list
                file_paths.append(os.path.join(root, file))

    print(f"Found {len(file_paths)} TXT files.")
    return file_paths

def load_files(file_paths):
    loaders = [
    TextLoader(filepath) for filepath in file_paths]
    docs = []
    for loader in tqdm(loaders, desc="Loading files...", leave=False):
        docs.extend(loader.load())
    
    print(f"{len(docs)} documents loaded.")
    return docs

def split_docs(docs):
    
    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
    )
    splits = text_splitter.split_documents(docs)

    print(f"There are {len(splits)} splits in total.")

    return splits

def create_vectorstore(splits):
    embedding = OpenAIEmbeddings(disallowed_special=())
    persist_directory = '/Users/kremerr/Documents/GitHub/RARR/triviaqa_vecdb'
    
    # Check if the vectordb already exists
    if os.path.exists(persist_directory) and os.listdir(persist_directory):
        print("Loading existing vectordb from:", persist_directory)
        vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
    else:
        try:
            print("Creating Chroma vector store...")
            vectordb = Chroma.from_documents(
                documents=splits,
                embedding=embedding,
                persist_directory=persist_directory,
                collection_name="trivia_qa_collection"
            )
        except openai.RateLimitError as e:
            print(f"RateLimitError: {e}. Waiting before retrying...")
            wait_time = 10  # Default wait time of 10 seconds
            time.sleep(wait_time)
        except openai.OpenAIError as e:
            print(f"OpenAIError: {e}.")
            raise
        except UnicodeEncodeError as e:
            start = max(e.start - 10, 0)
            end = min(e.end + 10, len(e.object))
            surrounding_text = e.object[start:end]
            print(f"UnicodeEncodeError: cannot encode text surrounding '{surrounding_text}' at position {e.start}-{e.end}")
            raise
    
    return vectordb

def main():
       
    """
    Full RAG-based Research and Revision pipeline (RAGR2)
    ------------------------------------------------------
    Main Hypothesis: 
        We can improve the speed and quality of hallucination mitigation by using a vector store RAG system instead of the web search-based approach in RARR.
    Use-case: 
        Mitigating hallucinations that happen at inference for RAG systems, and validating evidence retrieval.
        Can be used internally with proprietary data. 
        Can be completely air-gapped by replacing OpenAI calls with locally hosted LLM.
    ------------------------------------------------------
    RAGR2 evaluation dataset: 
        NarrativeQA (dataset for evaluating QA with RAG, long-form version of TriviaQA)
    Prep:
        Load and split evidence documents from NarrativeQA, vectorize splits and save to vector base (Chroma). This will be used both for Mistral input generation, and evidence retrieval in RAGR2.
        Develop a simple RAG QA chain with Mistral-7b-instruct-v0.1, to generate input text for RAGR2 AND RARR.
    TODO: Decide what metrics we are using to evaluate the hallucination (Refer to Google Sheet)
    Input: 
        Text generated by Mistral based on questions from NarrativeQA.
    ------------------------------------------------------
    1st stage - CQGen:
        Generate n queries covering all factual statements in the input text.
        Model: gpt-3.5-turbo-0125
    2nd stage - Evidence Retrieval:
        Generate an embedding of the query and retrieve the m (default m=1) most relevant documents from Chroma.
        Model: text-embedding-3-small
    3rd stage - Agreement gate:
        Pass the input text, query, and evidence to the agreement gate. If the evidence and the input provide disagreeing answers to the query, pass to the edit gate. Otherwise, move on to the next query, until all queries are complete.
        Model: gpt-3.5-turbo-0125
    4th stage - Revision:
        If a disagreement has been detected, pass the input text, query, and evidence to the edit model.
        Prompt the model to minimize the editing, penalizing high-change edits.
        TODO: Brainstorm how to remove iteration from editing phase (note: summarizing evidence can lose important details)
        Model: gpt-3.5-turbo-0125
    ------------------------------------------------------
    Evaluation:
        Measure the NarrativeQA accuracy/f1/precision/recall/BLEU/BERTScore before and after RARR.
        Compare the accuracy/etc. and preservation of the 'after' results for RAGR2 and RARR.
        Experiment with different RAG pipelines (e.g., different embeddings, splitters, retrievers, hyperparams) and compare their results using Ragas.
        TODO: Think of additional experiments.
    """
    

    file_paths = get_file_path()
    docs = load_files(file_paths)
    splits = split_docs(docs)
    vectordb = create_vectorstore(splits)

    print(f"There are {vectordb._collection.count()} items in the collection.")


if __name__ == "__main__":
    main()

       

