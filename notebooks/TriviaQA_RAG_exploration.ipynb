{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Experimenting with LangChain for RAG. \n",
    "\n",
    "**Dataset**: TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. \n",
    "\n",
    "**Output**: Evidence that will be used in the Revision part of the Research & Revision framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "persist_directory = '/Users/kremerr/Documents/GitHub/RARR/triviaqa_vecdb'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/kremerr/Documents/GitHub/RARR/trivia_qa/evidence'\n",
    "trivia_qa_files = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            # Construct the full file path and add it to the list\n",
    "            trivia_qa_files.append(os.path.join(root, file))\n",
    "\n",
    "trivia_qa_files.sort()\n",
    "print(f\"Found {len(trivia_qa_files)} TXT files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/Documents/GitHub/RARR/evidence/web/0/0_836.txt\n",
      "1839\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(trivia_qa_files)):\n",
    "    if trivia_qa_files[i] =='/Users/kremerr/Documents/GitHub/RARR/evidence/web/0/0_836.txt':\n",
    "        print(trivia_qa_files[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    TextLoader(filepath) for filepath in trivia_qa_files]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"1. What word, extended from a more popular term, refers to a fictional book of between 20,000 and 50,000 words? - Jade Wright - Liverpool Echo\\nNews Opinion\\n1. What word, extended from a more popular term, refers to a fictional book of between 20,000 and 50,000 words?\\n2. Who wrote the 1855 poem The Charge of the Light Brigade?\\n\\xa0Share\\nGet daily updates directly to your inbox\\n+ Subscribe\\nCould not subscribe, try again laterInvalid Email\\n2. Who wrote the 1855 poem The Charge of the Light Brigade?\\n3. In 1960 the UK publishing ban was lifted on what 1928 book?\\n4. How many times would a quarto sheet be folded?\\n5. Who wrote the seminal 1936 self-help book How to Win Friends and Influence People?\\n6. Who in 1450 invented movable type, thus revolutionising printing?\\n7. Which Polish-born naturalised British novelist's real surname was Korzeniowski?\\n8. Which short-lived dramatist is regarded as the first great exponent of blank verse?\\n9. Who wrote the maxim “Cogito, ergo sum” (I think, therefore I am)?\\n10. Who was the youngest of the three Brontë writing sisters?\\nŠ\\nANSWERS: 1. Novella; 2.Š Alfred Lord Tennyson; 3. Lady Chatterley's Lover (by D H Lawrence); 4. Twice (to create four leaves); 5. Dale Carnegie; 6. ŠJohannes Gutenberg; 7. ŠJoseph Conrad; 8. Christopher Marlowe; 9. René Descartes; 10. Anne\\nLike us on Facebook\\n\" metadata={'source': '/Users/kremerr/Documents/GitHub/RARR/evidence/web/0/0_1000687.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What word, extended from a more popular term, refers to a fictional book of between 20,000 and 50,000 words? - Jade Wright - Liverpool Echo\n",
      "News Opinion\n",
      "1. What word, extended from a more popular term, refers to a fictional book of between 20,000 and 50,000 words?\n",
      "2. Who wrote the 1855 poem The Charge of the Light Brigade?\n",
      " Share\n",
      "Get daily updates directly to your inbox\n",
      "+ Subscribe\n",
      "Could not subscribe, try again laterInvalid Email\n",
      "2. Who wrote the 1855 poem The Charge of the Light Brigade?\n",
      "3. In 1960 the UK publishing ban was lifted on what 1928 book?\n",
      "4. How many times would a quarto sheet be folded?\n",
      "5. Who wrote the seminal 1936 self-help book How to Win Friends and Influence People?\n",
      "6. Who in 1450 invented movable type, thus revolutionising printing?\n",
      "7. Which Polish-born naturalised British novelist's real surname was Korzeniowski?\n",
      "8. Which short-lived dramatist is regarded as the first great exponent of blank verse?\n",
      "\n",
      "8. Which short-lived dramatist is regarded as the first great exponent of blank verse?\n",
      "9. Who wrote the maxim “Cogito, ergo sum” (I think, therefore I am)?\n",
      "10. Who was the youngest of the three Brontë writing sisters?\n",
      "Š\n",
      "ANSWERS: 1. Novella; 2.Š Alfred Lord Tennyson; 3. Lady Chatterley's Lover (by D H Lawrence); 4. Twice (to create four leaves); 5. Dale Carnegie; 6. ŠJohannes Gutenberg; 7. ŠJoseph Conrad; 8. Christopher Marlowe; 9. René Descartes; 10. Anne\n",
      "Like us on Facebook\n",
      "\n",
      "John Nash (Architect) - Pics, Videos, Dating, & News\n",
      "John Nash\n",
      "John Nash was a British architect responsible for much of the layout of Regency London.\n",
      "related links\n",
      "The Rally: Pivotal Position â Offensive Line Or Secondary?   The Independent Florida Alligator\n",
      "Google News - Aug 30, 2011\n",
      "'Think <mark>John Nash</mark> from âA Beautiful Mind,â only with football diagrams instead of equations. But it&#39;s precisely because Muschamp&#39;s brain is so big and the defense is so talented that the offensive line is key. If Florida can put up points at a decent'\n",
      "George Iv: The Rehabilitation Of Old Naughty   The Guardian\n",
      "Google News - Aug 28, 2011\n",
      "'The period&#39;s defining architect is <mark>John Nash</mark>, with his crazily clashing source material. He built stage sets for the Prince Regent&#39;s no-rules lifestyle: as well as the Royal Pavilion, he carried out endless and unaffordable remodellings of Carlton'\n",
      "Wozniacki Back In New Haven Final   The Hour\n",
      "Google News - Aug 27, 2011\n",
      "\n",
      "Wozniacki Back In New Haven Final   The Hour\n",
      "Google News - Aug 27, 2011\n",
      "'NEW HAVEN -- Caroline Wozniacki, the Princess of Yale, overcame a tight groin and forced back each and every challenge that came from the other side of the net, earning a straight set victory and a fourth straight trip to'\n",
      "Fundamental Steps To Save Our Economy   Tbo.Com\n",
      "Google News - Aug 26, 2011\n",
      "'By <mark>John Nash</mark>, Of Cabbages and Kings Seems as though most everyone (ie, those who are literate, at least) is more concerned about the sad state of our economy than about anything else, including drought, wars, famine,'\n",
      "Learn about the memorable moments in the evolution of John Nash.\n",
      "CHILDHOOD\n",
      "1752 Birth Nash was born during 1752 in Lambeth, south London, the son of a Welsh millwright also called John (1714â1772).\n",
      "TEENAGE\n",
      "1766 14 Years Old From 1766 or 67, John Nash trained with the architect Sir Robert Taylor; the apprenticeship was completed in 1775 or 1776.\n",
      "TWENTIES\n",
      "1775 - 1777 2  More Events\n",
      "\n",
      "TWENTIES\n",
      "1775 - 1777 2  More Events\n",
      "1775 23 Years Old On 28 April 1775, at the now demolished church of St Mary Newington, Nash married his first wife Jane Elizabeth Kerr, daughter of a surgeon. … Read More\n",
      "Initially he seems to have pursued a career as a surveyor, builder and carpenter. This gave him an income of around Â£300 a year. The couple set up home at Royal Row Lambeth. Read Less\n",
      "1777 25 Years Old He established his own architectural practice in 1777 as well as being in partnership with a timber merchant, Richard Heaviside. … Read More\n",
      "The couple had two children, both were baptised at St Mary-at-Lambeth, John on 9 June 1776 and Hugh on 28 April 1778. Read Less\n",
      "\n",
      "1778 26 Years Old In June 1778 \"By the ill conduct of his wife found it necessary to send her into Wales in order to work a reformation on her\", the cause of this appears to have been the claim that Jane Nash \"Had imposed two spurious children on him as his and her own, notwithstanding she had then never had any child\" and she had contracted several debts unknown to her husband, including one for milliners' bills of Â£300. … Read More\n",
      "The claim that Jane had faked her pregnancies and then passed babies she had acquired off as her own was brought before the Consistory court of the Bishop of London. His wife was sent to Aberavon to lodge with Nash's cousin Ann Morgan, but she developed a relationship with a local man Charles Charles. Read Less\n",
      "1779 27 Years Old In an attempt at reconciliation Jane returned to London in June 1779, but she continued to act extravagantly so he sent her to another cousin, Thomas Edwards of Neath. … Read More\n",
      "\n",
      "She gave birth just after Christmas, and acknowledged Charles Charles as the father. Read Less\n",
      "1781 29 Years Old In 1781 Nash instigated action against Jane for separation on grounds of adultery. The case was tried at Hereford in 1782, Charles who was found guilty was unable to pay the damages of Â£76 and subsequently died in prison. The divorce was finally read 26 January 1787. … Read More\n",
      "His career was initially unsuccessful and short-lived. After inheriting Â£1000 in 1778 from his uncle Thomas, he invested the money in building his first known independent works, 15â17 Bloomsbury Square and 66â71 Great Russell Street in Bloomsbury. Read Less\n",
      "THIRTIES\n",
      "1783 - 1784 2  More Events\n",
      "1783 31 Years Old But the property failed to let and he was declared bankrupt on 30 September 1783. … Read More\n",
      "\n",
      "His debts were Â£5000, including Â£2000 he had been lent by Robert Adam and his brothers.<br /><br /> A blue plaque commemorating Nash was placed on 66 Great Russell Street by English Heritage in 2013. Read Less\n",
      "1784 32 Years Old Nash left London in 1784 to live in Carmarthen, where his mother had retired to, her family being from the area.\n",
      "1785 33 Years Old In 1785 he and a local man Samuel Simon Saxon re-roofed the town's church for 600 Guineas. … Read More\n",
      "Nash and Saxon seem to have worked as building contractors and suppliers of building materials. Nash's London buildings had been standard Georgian terrace houses, and it was in Wales that he matured as an architect. Read Less\n",
      "Show Less\n",
      "His first major work in the area was the first of three prisons he would design, Carmarthen 1789â92, this prison was planned by the penal reformer John Howard and Nash developed this into the finished building. … Read More\n",
      "\n",
      "He went on to design the prisons at Cardigan (1791â96) and Hereford (1792â96). It was at Hereford that Nash met Richard Payne Knight, whose theories on the picturesque as applies to architecture and landscape would influence Nash. The commission for Hereford Gaol came after the death of William Blackburn, who was to have designed the building, Nash's design was accepted after James Wyatt approved of the design. Read Less\n",
      "By 1789 St David's Cathedral was suffering from structural problems, the west front was leaning forward by one foot, Nash was called in to survey the structure and develop a plan to save the building, his solution completed in 1791 was to demolish the upper part of the facade and rebuild it with two large but inelegant flying buttresses.\n",
      "1790 38 Years Old In 1790 Nash met Uvedale Price, whose theories of the Picturesque would have a major future influence on Nash's town planning. … Read More\n",
      "\n",
      "In the short term Price would commission Nash to design Castle House Aberystwyth (1795). Its plan took the form of a rightangled triangle, with an octagonal tower at each corner, sited on the very edge of the sea. This marked a new and more imaginative approach to design in Nash's work. One of Nash's most important developments were a series of medium-sized country houses that he designed in Wales, these developed the villa designs of his teacher Sir Robert Taylor. Most of these villas consist of a roughly square plan with a small entrance hall with a staircase offset in the middle to one side, around which are placed the main rooms, there is then a less prominent Servants' quarters in a wing attached to one side of the villa. The buildings are usually only two floors in height, the elevations of the main block are usually symmetrical. One of the finest of these villas is Llanerchaeron, at least a dozen villas were designed throughout south Wales. Others, in Pembrokeshire, include\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(splits[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9139547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vectorstore using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450307602799495"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = splits[0].page_content\n",
    "sentence2 = splits[1].page_content\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    persist_directory = '/Users/kremerr/Documents/GitHub/RARR/triviaqa_vecdb'\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=\"trivia_qa_collection\"\n",
    "    )\n",
    "except openai.RateLimitError as e:\n",
    "                print(f\"RateLimitError: {e}. Waiting before retrying...\")\n",
    "                wait_time = 10  # Default wait time of 10 seconds\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "except openai.OpenAIError as e:\n",
    "                print(f\"OpenAIError: {e}.\")\n",
    "                raise\n",
    "\n",
    "except UnicodeEncodeError as e:\n",
    "                start = max(e.start - 10, 0)\n",
    "                end = min(e.end + 10, len(e.object))\n",
    "                surrounding_text = e.object[start:end]\n",
    "                print(f\"UnicodeEncodeError: cannot encode text surrounding '{surrounding_text}' at position {e.start}-{e.end}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22747\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the attention mechanism in a transformer model?\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\n",
      "2.1 T RANSFORMER ARCHITECTURE\n",
      "The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\n",
      "been developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\n",
      "While some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\n",
      "module of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\n",
      "the specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\n",
      "which is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\n",
      "attention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\n",
      "role of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\n",
      "2.1.1 A TTENTION MECHANISM\n",
      "The attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\n",
      "concentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "emphasize important parts based on their contextual relevance in the application. This technique yielded significant outcomes\n",
      "when implemented in computer vision, thereby promoting its widespread adoption in various other fields such as language\n",
      "processing.\n",
      "In 2017, a novel attention-based neural network, named “Transformer”, was introduced to address the limitations of other\n",
      "neural networks (such as A recurrent neural network (RNN)) in encoding long-range dependencies in sequences, particularly\n",
      "in language translation tasks (Vaswani et al., 2017). The incorporation of a self-attention mechanism in the transformer model\n",
      "improved the performance of the attention mechanism by better capturing local features and reducing the reliance on external\n",
      "information. In the original transformer architecture, the attention technique is implemented through the “Scaled Dot Product\n",
      "Attention”, which is based on three primary parameter matrices: Query (Q), Key (K), and Value (V). Each of these matrices\n",
      "carries an encoded representation of each input in the sequence (Vaswani et al., 2017). The SoftMax function is applied to\n",
      "obtain the final output of the attention process, which is a probability score computed from the combination of the weights of\n",
      "the three matrices (see Figure 1). Mathematically, the scaled dot product attention function is computed as follows:\n",
      "\n",
      "can also improve training stability in BLOOM [69].\n",
      "Attention and Bias. Attention mechanism is a critical com-\n",
      "ponent of Transformer. It allows the tokens across the se-\n",
      "quence to interact with each other and compute the repre-\n",
      "sentations of the input and output sequence.\n",
      "•Full attention . In the vanilla Transformer [22], the atten-\n",
      "tion mechanism is conducted in a pairwise way, considering\n",
      "the relations between all token pairs in a sequence. It adopts\n",
      "scaled dot-product attention, in which the hidden states\n",
      "are mapped into queries, keys, and values. Additionally,Transformer uses multi-head attention instead of single\n",
      "attention, projecting the queries, keys, and values with\n",
      "different projections in different heads. The concatenation\n",
      "of the output of each head is taken as the final output.\n",
      "•Sparse attention . A crucial challenge of full attention\n",
      "is the quadratic computational complexity, which becomes\n",
      "a burden when dealing with long sequences. Therefore,\n",
      "various efficient Transformer variants are proposed to re-\n",
      "duce the computational complexity of the attention mecha-\n",
      "nism [210, 211]. For instance, locally banded sparse attention\n",
      "(i.e.,Factorized Attention [212] has been adopted in GPT-\n",
      "3 [55]. Instead of the whole sequence, each query can only\n",
      "attend to a subset of tokens based on the positions.\n",
      "•Multi-query attention . Multi-query attention refers to the\n",
      "attention variant where different heads share the same linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\n",
      "2.1 T RANSFORMER ARCHITECTURE\n",
      "The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\n",
      "been developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\n",
      "While some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\n",
      "module of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\n",
      "the specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\n",
      "which is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\n",
      "attention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\n",
      "role of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\n",
      "2.1.1 A TTENTION MECHANISM\n",
      "The attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\n",
      "concentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "\n",
      "can also improve training stability in BLOOM [69].\n",
      "Attention and Bias. Attention mechanism is a critical com-\n",
      "ponent of Transformer. It allows the tokens across the se-\n",
      "quence to interact with each other and compute the repre-\n",
      "sentations of the input and output sequence.\n",
      "•Full attention . In the vanilla Transformer [22], the atten-\n",
      "tion mechanism is conducted in a pairwise way, considering\n",
      "the relations between all token pairs in a sequence. It adopts\n",
      "scaled dot-product attention, in which the hidden states\n",
      "are mapped into queries, keys, and values. Additionally,Transformer uses multi-head attention instead of single\n",
      "attention, projecting the queries, keys, and values with\n",
      "different projections in different heads. The concatenation\n",
      "of the output of each head is taken as the final output.\n",
      "•Sparse attention . A crucial challenge of full attention\n",
      "is the quadratic computational complexity, which becomes\n",
      "a burden when dealing with long sequences. Therefore,\n",
      "various efficient Transformer variants are proposed to re-\n",
      "duce the computational complexity of the attention mecha-\n",
      "nism [210, 211]. For instance, locally banded sparse attention\n",
      "(i.e.,Factorized Attention [212] has been adopted in GPT-\n",
      "3 [55]. Instead of the whole sequence, each query can only\n",
      "attend to a subset of tokens based on the positions.\n",
      "•Multi-query attention . Multi-query attention refers to the\n",
      "attention variant where different heads share the same linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__4c9a3644dee14218912f9ad032923e90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was the first name of the 22nd president of the United States of America?\" #\"What is a good replacement for eggs in baking?\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"<human>: Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "### CONTEXT\n",
    "{context}\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\\n\n",
    "<bot>:\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
