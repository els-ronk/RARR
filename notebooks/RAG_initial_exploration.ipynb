{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Experimenting with LangChain for RAG. \n",
    "Dataset: 277 ArXiV papers in .pdf format. \n",
    "Output: Evidence that will be used in the Revision part of the Research & Revision framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 277 PDF files.\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/kremerr/Documents/GitHub/RARR/archive'\n",
    "pdf_files = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            # Construct the full file path and add it to the list\n",
    "            pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "pdf_files.sort()\n",
    "print(f\"Found {len(pdf_files)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(pdf_files)):\n",
    "    if pdf_files[i] =='/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf':\n",
    "        print(pdf_files[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(filepath) for filepath in pdf_files]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='independent evaluation where raters assessed the quality of individual report findings. Prior to performing the\\nfinal evaluation, we iterated upon the instructions for the raters and calibrated their grades using a pilot set\\nof 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases,\\nwhere each case was rated by a single radiologist randomly selected from a pool of four. For independent\\nevaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M\\nmodel variants (12B, 84B, and 562B) for every case in the evaluation set. Radiologists were blind to the\\nsource of the report findings for all evaluation tasks, and the reports were presented in a randomized order.\\nSide-by-side evaluation The input to each side-by-side evaluation was a single chest X-ray, along with the\\n“indication” section from the MIMIC-CXR study. Four alternative options for the “findings” section of the\\nreport were shown to raters as depicted in Figure A.3. The four alternative “findings” sections corresponded\\nto the dataset reference report’s findings, and findings generated by three Med-PaLM M model variants (12B,\\n84B, 562B). Raters were asked to rank the four alternative findings based on their overall quality using their\\nbest clinical judgement.\\nIndependent evaluation For independent evaluation, raters were also presented with a single chest X-ray,\\nalong with the indication and reference report’s findings from the MIMIC-CXR study (marked explicitly as\\nsuch), but this time only a single findings paragraph generated by Med-PaLM M as shown in Figure A.4.\\nRaters were asked to assess the quality of the Med-PaLM M generated findings in the presence of the reference\\ninputs provided and their own judgement of the chest X-ray image. The rating schema proposed in Yu et al.\\n[60] served as inspiration for our evaluation task design.\\nFirst, raters assessed whether the quality and view of the provided image were sufficient to perform the\\nevaluation task fully. Next, they annotated all passages in the model-generated findings that they disagreed\\nwith (errors), and all missing parts (omissions). Raters categorized each error passage by its type (no finding,\\nincorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical\\nsignificance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters\\nspecified a passage that should have been included and determined if the omission had any clinical significance.\\n6 Results\\nHere we present results across the three different evaluation setups introduced in Section 5.\\n6.1 Med-PaLM M performs near or exceeding SOTA on all MultiMedBench tasks\\nMed-PaLM M performance versus baselines We compared Med-PaLM M with two baselines:\\n•prior SOTA specialist models for each of the MultiMedBench tasks\\n•a baseline generalist model (PaLM-E 84B) without any biomedical domain finetuning. We used this\\nmodel size variant (and not PaLM-E 562B) due to compute constraints.\\nResults are summarized in Table 2. Across MultiMedBench tasks, Med-PaLM M’s best result (across three\\nmodel sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior\\nSOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with\\na generalist model using the same set of model weights without any task-specific architecture customization\\nor optimization.\\nOn medical question answering tasks, we compared against the SOTA Med-PaLM 2 results [61] and observed\\nhigher performance of Med-PaLM 2. However, when compared to the baseline PaLM model on which\\nMed-PaLM M was built, Med-PaLM M outperformed the previous best PaLM results [9] by a large margin in\\nthe same few-shot setting on all three question answering datasets.\\nFurther, when compared to PaLM-E 84B as a generalist baseline without biomedical domain finetuning, Med-\\nPaLM M exhibited performance improvements on all 14 tasks often by a significant margin, demonstrating the\\nimportance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM\\nM as a generalist biomedical AI model. We further describe the results in detail for each of the individual\\ntasks in Section A.3.\\n|9' metadata={'source': '/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf', 'page': 8}\n"
     ]
    }
   ],
   "source": [
    "print(docs[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Generalist Biomedical AI\n",
      "Tao Tu∗,‡, 1, Shekoofeh Azizi∗,‡, 2,\n",
      "Danny Driess2, Mike Schaekermann1, Mohamed Amin1, Pi-Chuan Chang1, Andrew Carroll1,\n",
      "Chuck Lau1, Ryutaro Tanno2, Ira Ktena2, Basil Mustafa2, Aakanksha Chowdhery2, Yun Liu1,\n",
      "Simon Kornblith2, David Fleet2, Philip Mansfield1, Sushant Prakash1, Renee Wong1, Sunny Virmani1,\n",
      "Christopher Semturs1, S Sara Mahdavi2, Bradley Green1, Ewa Dominowska1, Blaise Aguera y Arcas1,\n",
      "Joelle Barral2, Dale Webster1, Greg S. Corrado1, Yossi Matias1, Karan Singhal1, Pete Florence2,\n",
      "Alan Karthikesalingam†,‡,1and Vivek Natarajan†,‡,1\n",
      "1Google Research,2Google DeepMind\n",
      "Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more.\n",
      "Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret\n",
      "this data at scale can potentially enable impactful applications ranging from scientific discovery to care\n",
      "delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal\n",
      "biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering,\n",
      "mammography and dermatology image interpretation, radiology report generation and summarization, and\n",
      "genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept\n",
      "for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly\n",
      "\n",
      "for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly\n",
      "encodes and interprets biomedical data including clinical language, imaging, and genomics with the same\n",
      "set of model weights . Med-PaLM M reaches performance competitive with or exceeding the state of the art\n",
      "on all MultiMedBench tasks, often surpassing specialist models by a wide margin. We also report examples\n",
      "of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and\n",
      "emergent zero-shot medical reasoning. To further probe the capabilities and limitations of Med-PaLM\n",
      "M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe\n",
      "encouraging performance across model scales. In a side-by-side ranking on 246 retrospective chest X-rays,\n",
      "clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up\n",
      "to 40.50% of cases, suggesting potential clinical utility. While considerable work is needed to validate these\n",
      "models in real-world use cases, our results represent a milestone towards the development of generalist\n",
      "biomedical AI systems.\n",
      "1 Introduction\n",
      "Medicine is a multimodal discipline. Clinicians routinely interpret data from a wide range of modalities\n",
      "including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, and more\n",
      "when providing care.\n",
      "\n",
      "including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, and more\n",
      "when providing care.\n",
      "Despite significant progress in biomedical AI, most models today are unimodal single task systems [1–3].\n",
      "Consider an existing AI system for interpreting mammograms [4]. Although the system obtains state-of-the-art\n",
      "(SOTA) performance on breast cancer screening, it cannot incorporate relevant information such as patient\n",
      "health records (e.g., breast cancer gene screening status), other modalities such as MRI, or published medical\n",
      "literature that might help contextualize, refine, and improve performance. Further, the system’s output is\n",
      "constrained to a pre-specified set of possible classifications. It cannot verbally explain its prediction or engage\n",
      "in a collaborative dialogue to learn from a physician’s feedback. This bounds performance and utility of these\n",
      "narrow, single-task, unimodal, specialist AI systems in real-world applications.\n",
      "The emergence of foundation models [5] offers an opportunity to rethink the development of medical AI\n",
      "systems. These models are often trained on large-scale data with self-supervised or unsupervised objectives\n",
      "and can be rapidly and effectively adapted to many downstream tasks and settings using in-context learning\n",
      "or few-shot finetuning [6, 7]. Further, they often have impressive generative capabilities that can enable\n",
      "\n",
      "or few-shot finetuning [6, 7]. Further, they often have impressive generative capabilities that can enable\n",
      "effective human-AI interaction and collaboration. These advances enable the possibility of building a unified\n",
      "biomedical AI system that can interpret multimodal data with complex structures to tackle many challenging\n",
      "∗Equal contributions. †Equal leadership.\n",
      "‡Corresponding authors: {taotu, shekazizi, alankarthi, natviv}@google.comarXiv:2307.14334v1  [cs.CL]  26 Jul 2023\n",
      "\n",
      "Med-PaLM M Genomics \n",
      "Radiograph \n",
      "Radiology \n",
      "Report \n",
      "Medical \n",
      "Knowledge \n",
      "Pathology \n",
      "Mammography Dermatology \n",
      "Medical \n",
      "Question \n",
      "Answering \n",
      "Medical Visual \n",
      "Question \n",
      "Answering \n",
      "Medical Image \n",
      "Classification \n",
      "Radiology \n",
      "Report \n",
      "Generation \n",
      "Genomic \n",
      "Variant Calling \n",
      "Radiology \n",
      "Report \n",
      "Summarization \n",
      "   MultiMedBench modalities and tasks \n",
      "Genomic \n",
      "Variant Calling \n",
      "Radio Report \n",
      "Generation \n",
      "Radiology Report \n",
      "Summarization Medical \n",
      "Question Answering \n",
      "Visual Question \n",
      "Answering Mammography \n",
      "Classification \n",
      "Dermatology \n",
      "Classification \n",
      "Best Prior Specialist Model Capability \n",
      " Med-PaLM M Capability \n",
      "Figure 1 |Med-PaLM M overview. A generalist biomedical AI system should be able to handle a diverse range of biomedical\n",
      "data modalities and tasks. To enable progress towards this overarching goal, we curate MultiMedBench, a benchmark spanning 14\n",
      "diverse biomedical tasks including question answering, visual question answering, image classification, radiology report generation\n",
      "and summarization, and genomic variant calling. Med-PaLM Multimodal (Med-PaLM M), our proof of concept for such a\n",
      "generalist biomedical AI system (denoted by the shaded blue area) is competitive with or exceeds prior SOTA results from\n",
      "specialists models (denoted by dotted red lines) on all tasks in MultiMedBench. Notably, Med-PaLM M achieves this using a\n",
      "single set of model weights, without any task-specific customization.\n",
      "\n",
      "single set of model weights, without any task-specific customization.\n",
      "tasks. As the pace of biomedical data generation and innovation increases, so will the potential impact of\n",
      "such models, with a breadth of possible downstream applications spanning fundamental biomedical discovery\n",
      "to care delivery.\n",
      "In this work, we detail our progress towards such a generalist biomedical AI system - a unified model that\n",
      "can interpret multiple biomedical data modalities and handle many downstream tasks with the same set of\n",
      "model weights . One of the key challenges of this goal has been the absence of comprehensive multimodal\n",
      "medical benchmarks. To address this unmet need, we curate MultiMedBench, an open source multimodal\n",
      "medical benchmark spanning language, medical imaging, and genomics modalities with 14 diverse biomedical\n",
      "tasks including question answering, visual question answering, medical image classification, radiology report\n",
      "generation and summarization, and genomic variant calling.\n",
      "We leverage MultiMedBench to design and develop Med-PaLM Multimodal (Med-PaLM M), a large-scale\n",
      "generalist biomedical AI system building on the recent advances in language [8, 9] and multimodal foundation\n",
      "models [10, 11]. In particular, Med-PaLM M is a flexible multimodal sequence-to-sequence architecture\n",
      "that can easily incorporate and interleave various types of multimodal biomedical information. Further, the\n",
      "\n",
      "that can easily incorporate and interleave various types of multimodal biomedical information. Further, the\n",
      "expressiveness of the modality-agnostic language decoder enables the handling of various biomedical tasks in\n",
      "a simple generative framework with a unified training strategy.\n",
      "To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system\n",
      "that can interpret multimodal biomedical data and handle a diverse range of tasks with a single model.\n",
      "Med-PaLM M reaches performance competitive with or exceeding the state-of-the-art (SOTA) on all tasks in\n",
      "MultiMedBench, often surpassing specialized domain and task-specific models by a large margin. In particular,\n",
      "Med-PaLM M exceeds prior state-of-the-art on chest X-ray (CXR) report generation (MIMIC-CXR dataset)\n",
      "by over 8% on the common success metric (micro-F1) for clinical efficacy. On one of the medical visual\n",
      "question answering tasks (Slake-VQA [12]) in MultiMedBench, Med-PaLM M outperforms the prior SOTA\n",
      "results by over 10% on the BLEU-1 and F1 metrics.\n",
      "|2\n",
      "\n",
      "We perform ablation studies to understand the importance of scale in our generalist multimodal biomedical\n",
      "models and observe significant benefits for tasks that require higher-level language capabilities, such as medical\n",
      "(visual) question answering. Preliminary experiments also suggest evidence of zero-shot generalization to novel\n",
      "medical concepts and tasks across model scales, and emergent capabilities [13] such as zero-shot multimodal\n",
      "medical reasoning. We further perform radiologist evaluation of AI-generated chest X-ray reports and observe\n",
      "encouraging results across model scales.\n",
      "Overall, these results demonstrate the potential of generalist biomedical AI systems for medicine. However,\n",
      "significant work remains in terms of large-scale biomedical data access for training such models, validating\n",
      "performance in real world applications, and understanding the safety implications. We outline these key\n",
      "limitations and directions of future research in our study. To summarize, our key contributions are as follows:\n",
      "•CurationofMultiMedBench WeintroduceMultiMedBench, anewmultimodalbiomedicalbenchmark\n",
      "spanning multiple modalities including medical imaging, clinical text and genomics with 14 diverse tasks\n",
      "for training and evaluating generalist biomedical AI systems.\n",
      "•Med-PaLM M, the first demonstration of a generalist biomedical AI system We introduce\n",
      "Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image\n",
      "\n",
      "Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image\n",
      "classification, medical question answering, visual question answering, radiology report generation and\n",
      "summarization, genomic variant calling, and more with the same set of model weights. Med-PaLM\n",
      "M reaches performance competitive with or exceeding state-of-the-art (SOTA) specialist models on\n",
      "multiple tasks in MultiMedBench without any task-specific customization.\n",
      "•Evidence of novel emergent capabilities in Med-PaLM M Beyond quantitative evaluations of\n",
      "task performance, we observe evidence of zero-shot medical reasoning, generalization to novel medical\n",
      "concepts and tasks, and positive transfer across tasks. These experiments suggest promising potential of\n",
      "such systems in downstream data-scarce biomedical applications.\n",
      "•Human evaluation of Med-PaLM M outputs Beyond automated metrics, we perform radiologist\n",
      "evaluation of chest X-ray reports generated by Med-PaLM M across different model scales. In a blinded\n",
      "side-by-side ranking on 246 retrospective chest X-rays, clinicians expressed a pairwise preference for\n",
      "Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases. Furthermore, the\n",
      "best Med-PaLM M model has on average 0.25 clinically significant errors per report. These results are\n",
      "on par with human baselines from prior work [14], suggesting potential clinical utility.\n",
      "2 Related Work\n",
      "2.1 Foundation models, multimodality, and generalists\n",
      "\n",
      "2 Related Work\n",
      "2.1 Foundation models, multimodality, and generalists\n",
      "The emergence of the foundation model paradigm [5] has had widespread impact across a variety of\n",
      "applications in language [8], vision [15], and other modalities [16]. While the idea of transfer learning [17, 18]\n",
      "using the weights of pretrained models has existed for decades [19–22], a shift has come about due to the\n",
      "scale of data and compute used for pretraining such models [23]. The notion of a foundation model further\n",
      "indicates that the model can be adapted to a wide range of downstream tasks [5].\n",
      "Within the foundation model paradigm, multimodality [24] has also had a variety of important impacts\n",
      "– in the datasets [25], in the inter-modality supervision [26], and in the generality and unification of task\n",
      "specification [27, 28]. For example, language has specifically been an important enabler of foundation models\n",
      "in other modalities [11, 29]. Visual foundation models such as CLIP [30] are made possible by training\n",
      "on language-labeled visual datasets [25, 31], which are easier to collect from large-scale internet data than\n",
      "classification datasets with pre-determined class labels (i.e., ImageNet [32]). The benefits of joint language-\n",
      "and-vision supervision has also been noteworthy in generative modeling of images [33], where text-to-image\n",
      "generative modeling has been notably more successful at producing high-fidelity image generation [34] than\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(splits[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22747 splits in total.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {i} splits in total.\".format(i=len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vectorstore using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205597944928818"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = splits[0].page_content\n",
    "sentence2 = splits[1].page_content\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '/Users/kremerr/Documents/GitHub/RARR/chroma'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"langchain_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22747\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the attention mechanism in a transformer model?\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\n",
      "2.1 T RANSFORMER ARCHITECTURE\n",
      "The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\n",
      "been developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\n",
      "While some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\n",
      "module of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\n",
      "the specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\n",
      "which is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\n",
      "attention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\n",
      "role of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\n",
      "2.1.1 A TTENTION MECHANISM\n",
      "The attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\n",
      "concentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "emphasize important parts based on their contextual relevance in the application. This technique yielded significant outcomes\n",
      "when implemented in computer vision, thereby promoting its widespread adoption in various other fields such as language\n",
      "processing.\n",
      "In 2017, a novel attention-based neural network, named “Transformer”, was introduced to address the limitations of other\n",
      "neural networks (such as A recurrent neural network (RNN)) in encoding long-range dependencies in sequences, particularly\n",
      "in language translation tasks (Vaswani et al., 2017). The incorporation of a self-attention mechanism in the transformer model\n",
      "improved the performance of the attention mechanism by better capturing local features and reducing the reliance on external\n",
      "information. In the original transformer architecture, the attention technique is implemented through the “Scaled Dot Product\n",
      "Attention”, which is based on three primary parameter matrices: Query (Q), Key (K), and Value (V). Each of these matrices\n",
      "carries an encoded representation of each input in the sequence (Vaswani et al., 2017). The SoftMax function is applied to\n",
      "obtain the final output of the attention process, which is a probability score computed from the combination of the weights of\n",
      "the three matrices (see Figure 1). Mathematically, the scaled dot product attention function is computed as follows:\n",
      "\n",
      "can also improve training stability in BLOOM [69].\n",
      "Attention and Bias. Attention mechanism is a critical com-\n",
      "ponent of Transformer. It allows the tokens across the se-\n",
      "quence to interact with each other and compute the repre-\n",
      "sentations of the input and output sequence.\n",
      "•Full attention . In the vanilla Transformer [22], the atten-\n",
      "tion mechanism is conducted in a pairwise way, considering\n",
      "the relations between all token pairs in a sequence. It adopts\n",
      "scaled dot-product attention, in which the hidden states\n",
      "are mapped into queries, keys, and values. Additionally,Transformer uses multi-head attention instead of single\n",
      "attention, projecting the queries, keys, and values with\n",
      "different projections in different heads. The concatenation\n",
      "of the output of each head is taken as the final output.\n",
      "•Sparse attention . A crucial challenge of full attention\n",
      "is the quadratic computational complexity, which becomes\n",
      "a burden when dealing with long sequences. Therefore,\n",
      "various efficient Transformer variants are proposed to re-\n",
      "duce the computational complexity of the attention mecha-\n",
      "nism [210, 211]. For instance, locally banded sparse attention\n",
      "(i.e.,Factorized Attention [212] has been adopted in GPT-\n",
      "3 [55]. Instead of the whole sequence, each query can only\n",
      "attend to a subset of tokens based on the positions.\n",
      "•Multi-query attention . Multi-query attention refers to the\n",
      "attention variant where different heads share the same linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\n",
      "2.1 T RANSFORMER ARCHITECTURE\n",
      "The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\n",
      "been developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\n",
      "While some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\n",
      "module of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\n",
      "the specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\n",
      "which is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\n",
      "attention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\n",
      "role of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\n",
      "2.1.1 A TTENTION MECHANISM\n",
      "The attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\n",
      "concentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\n",
      "than others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\n",
      "\n",
      "can also improve training stability in BLOOM [69].\n",
      "Attention and Bias. Attention mechanism is a critical com-\n",
      "ponent of Transformer. It allows the tokens across the se-\n",
      "quence to interact with each other and compute the repre-\n",
      "sentations of the input and output sequence.\n",
      "•Full attention . In the vanilla Transformer [22], the atten-\n",
      "tion mechanism is conducted in a pairwise way, considering\n",
      "the relations between all token pairs in a sequence. It adopts\n",
      "scaled dot-product attention, in which the hidden states\n",
      "are mapped into queries, keys, and values. Additionally,Transformer uses multi-head attention instead of single\n",
      "attention, projecting the queries, keys, and values with\n",
      "different projections in different heads. The concatenation\n",
      "of the output of each head is taken as the final output.\n",
      "•Sparse attention . A crucial challenge of full attention\n",
      "is the quadratic computational complexity, which becomes\n",
      "a burden when dealing with long sequences. Therefore,\n",
      "various efficient Transformer variants are proposed to re-\n",
      "duce the computational complexity of the attention mecha-\n",
      "nism [210, 211]. For instance, locally banded sparse attention\n",
      "(i.e.,Factorized Attention [212] has been adopted in GPT-\n",
      "3 [55]. Instead of the whole sequence, each query can only\n",
      "attend to a subset of tokens based on the positions.\n",
      "•Multi-query attention . Multi-query attention refers to the\n",
      "attention variant where different heads share the same linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__4c9a3644dee14218912f9ad032923e90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What was the first name of the 22nd president of the United States of America?\" #\"What is a good replacement for eggs in baking?\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"<human>: Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "### CONTEXT\n",
    "{context}\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\\n\n",
    "<bot>:\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
