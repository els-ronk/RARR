{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Experimenting with LangChain for RAG. \n",
    "Dataset: 277 ArXiV papers in .pdf format. \n",
    "Output: Evidence that will be used in the Revision part of the Research & Revision framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 277 PDF files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'your_directory_path' with the path to the directory containing your PDF files\n",
    "folder_path = '/Users/kremerr/Documents/GitHub/RARR/archive'\n",
    "pdf_files = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            # Construct the full file path and add it to the list\n",
    "            pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "# pdf_files now contains the paths to all the PDF files in the folder and its subdirectories\n",
    "pdf_files.sort()\n",
    "print(f\"Found {len(pdf_files)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(pdf_files)):\n",
    "    if pdf_files[i] =='/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf':\n",
    "        print(pdf_files[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(filepath) for filepath in pdf_files]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='independent evaluation where raters assessed the quality of individual report findings. Prior to performing the\\nfinal evaluation, we iterated upon the instructions for the raters and calibrated their grades using a pilot set\\nof 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases,\\nwhere each case was rated by a single radiologist randomly selected from a pool of four. For independent\\nevaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M\\nmodel variants (12B, 84B, and 562B) for every case in the evaluation set. Radiologists were blind to the\\nsource of the report findings for all evaluation tasks, and the reports were presented in a randomized order.\\nSide-by-side evaluation The input to each side-by-side evaluation was a single chest X-ray, along with the\\n“indication” section from the MIMIC-CXR study. Four alternative options for the “findings” section of the\\nreport were shown to raters as depicted in Figure A.3. The four alternative “findings” sections corresponded\\nto the dataset reference report’s findings, and findings generated by three Med-PaLM M model variants (12B,\\n84B, 562B). Raters were asked to rank the four alternative findings based on their overall quality using their\\nbest clinical judgement.\\nIndependent evaluation For independent evaluation, raters were also presented with a single chest X-ray,\\nalong with the indication and reference report’s findings from the MIMIC-CXR study (marked explicitly as\\nsuch), but this time only a single findings paragraph generated by Med-PaLM M as shown in Figure A.4.\\nRaters were asked to assess the quality of the Med-PaLM M generated findings in the presence of the reference\\ninputs provided and their own judgement of the chest X-ray image. The rating schema proposed in Yu et al.\\n[60] served as inspiration for our evaluation task design.\\nFirst, raters assessed whether the quality and view of the provided image were sufficient to perform the\\nevaluation task fully. Next, they annotated all passages in the model-generated findings that they disagreed\\nwith (errors), and all missing parts (omissions). Raters categorized each error passage by its type (no finding,\\nincorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical\\nsignificance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters\\nspecified a passage that should have been included and determined if the omission had any clinical significance.\\n6 Results\\nHere we present results across the three different evaluation setups introduced in Section 5.\\n6.1 Med-PaLM M performs near or exceeding SOTA on all MultiMedBench tasks\\nMed-PaLM M performance versus baselines We compared Med-PaLM M with two baselines:\\n•prior SOTA specialist models for each of the MultiMedBench tasks\\n•a baseline generalist model (PaLM-E 84B) without any biomedical domain finetuning. We used this\\nmodel size variant (and not PaLM-E 562B) due to compute constraints.\\nResults are summarized in Table 2. Across MultiMedBench tasks, Med-PaLM M’s best result (across three\\nmodel sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior\\nSOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with\\na generalist model using the same set of model weights without any task-specific architecture customization\\nor optimization.\\nOn medical question answering tasks, we compared against the SOTA Med-PaLM 2 results [61] and observed\\nhigher performance of Med-PaLM 2. However, when compared to the baseline PaLM model on which\\nMed-PaLM M was built, Med-PaLM M outperformed the previous best PaLM results [9] by a large margin in\\nthe same few-shot setting on all three question answering datasets.\\nFurther, when compared to PaLM-E 84B as a generalist baseline without biomedical domain finetuning, Med-\\nPaLM M exhibited performance improvements on all 14 tasks often by a significant margin, demonstrating the\\nimportance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM\\nM as a generalist biomedical AI model. We further describe the results in detail for each of the individual\\ntasks in Section A.3.\\n|9' metadata={'source': '/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf', 'page': 8}\n"
     ]
    }
   ],
   "source": [
    "print(docs[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
