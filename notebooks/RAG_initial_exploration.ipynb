{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Experimenting with LangChain for RAG. \n",
    "Dataset: 277 ArXiV papers in .pdf format. \n",
    "Output: Evidence that will be used in the Revision part of the Research & Revision framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 277 PDF files.\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/kremerr/Documents/GitHub/RARR/archive'\n",
    "pdf_files = []\n",
    "\n",
    "# Walk through the directory\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            # Construct the full file path and add it to the list\n",
    "            pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "pdf_files.sort()\n",
    "print(f\"Found {len(pdf_files)} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(pdf_files)):\n",
    "    if pdf_files[i] =='/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf':\n",
    "        print(pdf_files[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.0000000000-170985' : FloatObject (b'0.0000000000-170985') invalid; use 0.0 instead\n"
     ]
    }
   ],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(filepath) for filepath in pdf_files]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='independent evaluation where raters assessed the quality of individual report findings. Prior to performing the\\nfinal evaluation, we iterated upon the instructions for the raters and calibrated their grades using a pilot set\\nof 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases,\\nwhere each case was rated by a single radiologist randomly selected from a pool of four. For independent\\nevaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M\\nmodel variants (12B, 84B, and 562B) for every case in the evaluation set. Radiologists were blind to the\\nsource of the report findings for all evaluation tasks, and the reports were presented in a randomized order.\\nSide-by-side evaluation The input to each side-by-side evaluation was a single chest X-ray, along with the\\n“indication” section from the MIMIC-CXR study. Four alternative options for the “findings” section of the\\nreport were shown to raters as depicted in Figure A.3. The four alternative “findings” sections corresponded\\nto the dataset reference report’s findings, and findings generated by three Med-PaLM M model variants (12B,\\n84B, 562B). Raters were asked to rank the four alternative findings based on their overall quality using their\\nbest clinical judgement.\\nIndependent evaluation For independent evaluation, raters were also presented with a single chest X-ray,\\nalong with the indication and reference report’s findings from the MIMIC-CXR study (marked explicitly as\\nsuch), but this time only a single findings paragraph generated by Med-PaLM M as shown in Figure A.4.\\nRaters were asked to assess the quality of the Med-PaLM M generated findings in the presence of the reference\\ninputs provided and their own judgement of the chest X-ray image. The rating schema proposed in Yu et al.\\n[60] served as inspiration for our evaluation task design.\\nFirst, raters assessed whether the quality and view of the provided image were sufficient to perform the\\nevaluation task fully. Next, they annotated all passages in the model-generated findings that they disagreed\\nwith (errors), and all missing parts (omissions). Raters categorized each error passage by its type (no finding,\\nincorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical\\nsignificance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters\\nspecified a passage that should have been included and determined if the omission had any clinical significance.\\n6 Results\\nHere we present results across the three different evaluation setups introduced in Section 5.\\n6.1 Med-PaLM M performs near or exceeding SOTA on all MultiMedBench tasks\\nMed-PaLM M performance versus baselines We compared Med-PaLM M with two baselines:\\n•prior SOTA specialist models for each of the MultiMedBench tasks\\n•a baseline generalist model (PaLM-E 84B) without any biomedical domain finetuning. We used this\\nmodel size variant (and not PaLM-E 562B) due to compute constraints.\\nResults are summarized in Table 2. Across MultiMedBench tasks, Med-PaLM M’s best result (across three\\nmodel sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior\\nSOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with\\na generalist model using the same set of model weights without any task-specific architecture customization\\nor optimization.\\nOn medical question answering tasks, we compared against the SOTA Med-PaLM 2 results [61] and observed\\nhigher performance of Med-PaLM 2. However, when compared to the baseline PaLM model on which\\nMed-PaLM M was built, Med-PaLM M outperformed the previous best PaLM results [9] by a large margin in\\nthe same few-shot setting on all three question answering datasets.\\nFurther, when compared to PaLM-E 84B as a generalist baseline without biomedical domain finetuning, Med-\\nPaLM M exhibited performance improvements on all 14 tasks often by a significant margin, demonstrating the\\nimportance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM\\nM as a generalist biomedical AI model. We further describe the results in detail for each of the individual\\ntasks in Section A.3.\\n|9' metadata={'source': '/Users/kremerr/Documents/GitHub/RARR/archive/2307.14334.pdf', 'page': 8}\n"
     ]
    }
   ],
   "source": [
    "print(docs[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Generalist Biomedical AI\n",
      "Tao Tu∗,‡, 1, Shekoofeh Azizi∗,‡, 2,\n",
      "Danny Driess2, Mike Schaekermann1, Mohamed Amin1, Pi-Chuan Chang1, Andrew Carroll1,\n",
      "Chuck Lau1, Ryutaro Tanno2, Ira Ktena2, Basil Mustafa2, Aakanksha Chowdhery2, Yun Liu1,\n",
      "Simon Kornblith2, David Fleet2, Philip Mansfield1, Sushant Prakash1, Renee Wong1, Sunny Virmani1,\n",
      "Christopher Semturs1, S Sara Mahdavi2, Bradley Green1, Ewa Dominowska1, Blaise Aguera y Arcas1,\n",
      "Joelle Barral2, Dale Webster1, Greg S. Corrado1, Yossi Matias1, Karan Singhal1, Pete Florence2,\n",
      "Alan Karthikesalingam†,‡,1and Vivek Natarajan†,‡,1\n",
      "1Google Research,2Google DeepMind\n",
      "Medicine is inherently multimodal, with rich data modalities spanning text, imaging, genomics, and more.\n",
      "Generalist biomedical artificial intelligence (AI) systems that flexibly encode, integrate, and interpret\n",
      "this data at scale can potentially enable impactful applications ranging from scientific discovery to care\n",
      "delivery. To enable the development of these models, we first curate MultiMedBench, a new multimodal\n",
      "biomedical benchmark. MultiMedBench encompasses 14 diverse tasks such as medical question answering,\n",
      "mammography and dermatology image interpretation, radiology report generation and summarization, and\n",
      "genomic variant calling. We then introduce Med-PaLM Multimodal (Med-PaLM M), our proof of concept\n",
      "for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly\n",
      "\n",
      "for a generalist biomedical AI system. Med-PaLM M is a large multimodal generative model that flexibly\n",
      "encodes and interprets biomedical data including clinical language, imaging, and genomics with the same\n",
      "set of model weights . Med-PaLM M reaches performance competitive with or exceeding the state of the art\n",
      "on all MultiMedBench tasks, often surpassing specialist models by a wide margin. We also report examples\n",
      "of zero-shot generalization to novel medical concepts and tasks, positive transfer learning across tasks, and\n",
      "emergent zero-shot medical reasoning. To further probe the capabilities and limitations of Med-PaLM\n",
      "M, we conduct a radiologist evaluation of model-generated (and human) chest X-ray reports and observe\n",
      "encouraging performance across model scales. In a side-by-side ranking on 246 retrospective chest X-rays,\n",
      "clinicians express a pairwise preference for Med-PaLM M reports over those produced by radiologists in up\n",
      "to 40.50% of cases, suggesting potential clinical utility. While considerable work is needed to validate these\n",
      "models in real-world use cases, our results represent a milestone towards the development of generalist\n",
      "biomedical AI systems.\n",
      "1 Introduction\n",
      "Medicine is a multimodal discipline. Clinicians routinely interpret data from a wide range of modalities\n",
      "including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, and more\n",
      "when providing care.\n",
      "\n",
      "including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, and more\n",
      "when providing care.\n",
      "Despite significant progress in biomedical AI, most models today are unimodal single task systems [1–3].\n",
      "Consider an existing AI system for interpreting mammograms [4]. Although the system obtains state-of-the-art\n",
      "(SOTA) performance on breast cancer screening, it cannot incorporate relevant information such as patient\n",
      "health records (e.g., breast cancer gene screening status), other modalities such as MRI, or published medical\n",
      "literature that might help contextualize, refine, and improve performance. Further, the system’s output is\n",
      "constrained to a pre-specified set of possible classifications. It cannot verbally explain its prediction or engage\n",
      "in a collaborative dialogue to learn from a physician’s feedback. This bounds performance and utility of these\n",
      "narrow, single-task, unimodal, specialist AI systems in real-world applications.\n",
      "The emergence of foundation models [5] offers an opportunity to rethink the development of medical AI\n",
      "systems. These models are often trained on large-scale data with self-supervised or unsupervised objectives\n",
      "and can be rapidly and effectively adapted to many downstream tasks and settings using in-context learning\n",
      "or few-shot finetuning [6, 7]. Further, they often have impressive generative capabilities that can enable\n",
      "\n",
      "or few-shot finetuning [6, 7]. Further, they often have impressive generative capabilities that can enable\n",
      "effective human-AI interaction and collaboration. These advances enable the possibility of building a unified\n",
      "biomedical AI system that can interpret multimodal data with complex structures to tackle many challenging\n",
      "∗Equal contributions. †Equal leadership.\n",
      "‡Corresponding authors: {taotu, shekazizi, alankarthi, natviv}@google.comarXiv:2307.14334v1  [cs.CL]  26 Jul 2023\n",
      "\n",
      "Med-PaLM M Genomics \n",
      "Radiograph \n",
      "Radiology \n",
      "Report \n",
      "Medical \n",
      "Knowledge \n",
      "Pathology \n",
      "Mammography Dermatology \n",
      "Medical \n",
      "Question \n",
      "Answering \n",
      "Medical Visual \n",
      "Question \n",
      "Answering \n",
      "Medical Image \n",
      "Classification \n",
      "Radiology \n",
      "Report \n",
      "Generation \n",
      "Genomic \n",
      "Variant Calling \n",
      "Radiology \n",
      "Report \n",
      "Summarization \n",
      "   MultiMedBench modalities and tasks \n",
      "Genomic \n",
      "Variant Calling \n",
      "Radio Report \n",
      "Generation \n",
      "Radiology Report \n",
      "Summarization Medical \n",
      "Question Answering \n",
      "Visual Question \n",
      "Answering Mammography \n",
      "Classification \n",
      "Dermatology \n",
      "Classification \n",
      "Best Prior Specialist Model Capability \n",
      " Med-PaLM M Capability \n",
      "Figure 1 |Med-PaLM M overview. A generalist biomedical AI system should be able to handle a diverse range of biomedical\n",
      "data modalities and tasks. To enable progress towards this overarching goal, we curate MultiMedBench, a benchmark spanning 14\n",
      "diverse biomedical tasks including question answering, visual question answering, image classification, radiology report generation\n",
      "and summarization, and genomic variant calling. Med-PaLM Multimodal (Med-PaLM M), our proof of concept for such a\n",
      "generalist biomedical AI system (denoted by the shaded blue area) is competitive with or exceeds prior SOTA results from\n",
      "specialists models (denoted by dotted red lines) on all tasks in MultiMedBench. Notably, Med-PaLM M achieves this using a\n",
      "single set of model weights, without any task-specific customization.\n",
      "\n",
      "single set of model weights, without any task-specific customization.\n",
      "tasks. As the pace of biomedical data generation and innovation increases, so will the potential impact of\n",
      "such models, with a breadth of possible downstream applications spanning fundamental biomedical discovery\n",
      "to care delivery.\n",
      "In this work, we detail our progress towards such a generalist biomedical AI system - a unified model that\n",
      "can interpret multiple biomedical data modalities and handle many downstream tasks with the same set of\n",
      "model weights . One of the key challenges of this goal has been the absence of comprehensive multimodal\n",
      "medical benchmarks. To address this unmet need, we curate MultiMedBench, an open source multimodal\n",
      "medical benchmark spanning language, medical imaging, and genomics modalities with 14 diverse biomedical\n",
      "tasks including question answering, visual question answering, medical image classification, radiology report\n",
      "generation and summarization, and genomic variant calling.\n",
      "We leverage MultiMedBench to design and develop Med-PaLM Multimodal (Med-PaLM M), a large-scale\n",
      "generalist biomedical AI system building on the recent advances in language [8, 9] and multimodal foundation\n",
      "models [10, 11]. In particular, Med-PaLM M is a flexible multimodal sequence-to-sequence architecture\n",
      "that can easily incorporate and interleave various types of multimodal biomedical information. Further, the\n",
      "\n",
      "that can easily incorporate and interleave various types of multimodal biomedical information. Further, the\n",
      "expressiveness of the modality-agnostic language decoder enables the handling of various biomedical tasks in\n",
      "a simple generative framework with a unified training strategy.\n",
      "To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system\n",
      "that can interpret multimodal biomedical data and handle a diverse range of tasks with a single model.\n",
      "Med-PaLM M reaches performance competitive with or exceeding the state-of-the-art (SOTA) on all tasks in\n",
      "MultiMedBench, often surpassing specialized domain and task-specific models by a large margin. In particular,\n",
      "Med-PaLM M exceeds prior state-of-the-art on chest X-ray (CXR) report generation (MIMIC-CXR dataset)\n",
      "by over 8% on the common success metric (micro-F1) for clinical efficacy. On one of the medical visual\n",
      "question answering tasks (Slake-VQA [12]) in MultiMedBench, Med-PaLM M outperforms the prior SOTA\n",
      "results by over 10% on the BLEU-1 and F1 metrics.\n",
      "|2\n",
      "\n",
      "We perform ablation studies to understand the importance of scale in our generalist multimodal biomedical\n",
      "models and observe significant benefits for tasks that require higher-level language capabilities, such as medical\n",
      "(visual) question answering. Preliminary experiments also suggest evidence of zero-shot generalization to novel\n",
      "medical concepts and tasks across model scales, and emergent capabilities [13] such as zero-shot multimodal\n",
      "medical reasoning. We further perform radiologist evaluation of AI-generated chest X-ray reports and observe\n",
      "encouraging results across model scales.\n",
      "Overall, these results demonstrate the potential of generalist biomedical AI systems for medicine. However,\n",
      "significant work remains in terms of large-scale biomedical data access for training such models, validating\n",
      "performance in real world applications, and understanding the safety implications. We outline these key\n",
      "limitations and directions of future research in our study. To summarize, our key contributions are as follows:\n",
      "•CurationofMultiMedBench WeintroduceMultiMedBench, anewmultimodalbiomedicalbenchmark\n",
      "spanning multiple modalities including medical imaging, clinical text and genomics with 14 diverse tasks\n",
      "for training and evaluating generalist biomedical AI systems.\n",
      "•Med-PaLM M, the first demonstration of a generalist biomedical AI system We introduce\n",
      "Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image\n",
      "\n",
      "Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image\n",
      "classification, medical question answering, visual question answering, radiology report generation and\n",
      "summarization, genomic variant calling, and more with the same set of model weights. Med-PaLM\n",
      "M reaches performance competitive with or exceeding state-of-the-art (SOTA) specialist models on\n",
      "multiple tasks in MultiMedBench without any task-specific customization.\n",
      "•Evidence of novel emergent capabilities in Med-PaLM M Beyond quantitative evaluations of\n",
      "task performance, we observe evidence of zero-shot medical reasoning, generalization to novel medical\n",
      "concepts and tasks, and positive transfer across tasks. These experiments suggest promising potential of\n",
      "such systems in downstream data-scarce biomedical applications.\n",
      "•Human evaluation of Med-PaLM M outputs Beyond automated metrics, we perform radiologist\n",
      "evaluation of chest X-ray reports generated by Med-PaLM M across different model scales. In a blinded\n",
      "side-by-side ranking on 246 retrospective chest X-rays, clinicians expressed a pairwise preference for\n",
      "Med-PaLM M reports over those produced by radiologists in up to 40.50% of cases. Furthermore, the\n",
      "best Med-PaLM M model has on average 0.25 clinically significant errors per report. These results are\n",
      "on par with human baselines from prior work [14], suggesting potential clinical utility.\n",
      "2 Related Work\n",
      "2.1 Foundation models, multimodality, and generalists\n",
      "\n",
      "2 Related Work\n",
      "2.1 Foundation models, multimodality, and generalists\n",
      "The emergence of the foundation model paradigm [5] has had widespread impact across a variety of\n",
      "applications in language [8], vision [15], and other modalities [16]. While the idea of transfer learning [17, 18]\n",
      "using the weights of pretrained models has existed for decades [19–22], a shift has come about due to the\n",
      "scale of data and compute used for pretraining such models [23]. The notion of a foundation model further\n",
      "indicates that the model can be adapted to a wide range of downstream tasks [5].\n",
      "Within the foundation model paradigm, multimodality [24] has also had a variety of important impacts\n",
      "– in the datasets [25], in the inter-modality supervision [26], and in the generality and unification of task\n",
      "specification [27, 28]. For example, language has specifically been an important enabler of foundation models\n",
      "in other modalities [11, 29]. Visual foundation models such as CLIP [30] are made possible by training\n",
      "on language-labeled visual datasets [25, 31], which are easier to collect from large-scale internet data than\n",
      "classification datasets with pre-determined class labels (i.e., ImageNet [32]). The benefits of joint language-\n",
      "and-vision supervision has also been noteworthy in generative modeling of images [33], where text-to-image\n",
      "generative modeling has been notably more successful at producing high-fidelity image generation [34] than\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(splits[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a vectorstore using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9205249452627864"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = splits[0].page_content\n",
    "sentence2 = splits[1].page_content\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "\n",
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kremerr/Documents/GitHub/RARR/notebooks'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kremerr/Documents/GitHub/RARR/chroma\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    735\u001b[0m     ):\n\u001b[0;32m--> 736\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:297\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_with_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/chromadb/api/models/Collection.py:487\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/chromadb/api/segment.py:463\u001b[0m, in \u001b[0;36mSegmentAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record(coll, r)\n\u001b[1;32m    462\u001b[0m     records_to_submit\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_producer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords_to_submit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/rarr/lib/python3.10/site-packages/chromadb/db/mixins/embeddings_queue.py:172\u001b[0m, in \u001b[0;36mSqlEmbeddingsQueue.submit_embeddings\u001b[0;34m(self, topic_name, embeddings)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# The returning clause does not guarantee order, so we need to do reorder\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# the results. https://www.sqlite.org/lang_returning.html\u001b[39;00m\n\u001b[1;32m    171\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RETURNING seq_id, id\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Pypika doesn't support RETURNING\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Reorder the results\u001b[39;00m\n\u001b[1;32m    174\u001b[0m seq_ids \u001b[38;5;241m=\u001b[39m [cast(SeqId, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    175\u001b[0m     results\n\u001b[1;32m    176\u001b[0m )  \u001b[38;5;66;03m# Lie to mypy: https://stackoverflow.com/questions/76694215/python-type-casting-when-preallocating-list\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "persist_directory = '/Users/kremerr/Documents/GitHub/RARR/chroma'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvectordb\u001b[49m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mcount())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the attention mechanism in a transformer model?\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Understanding Chain-of-Thought Prompting:\n",
      "An Empirical Study of What Matters\n",
      "Boshi Wang1Sewon Min2Xiang Deng1Jiaming Shen3You Wu3\n",
      "Luke Zettlemoyer2Huan Sun1\n",
      "1The Ohio State University2University of Washington3Google Research\n",
      "{wang.13930,deng.595,sun.397}@osu.edu\n",
      "{sewon,lsz}@cs.washington.edu ,{jmshen,wuyou}@google.com\n",
      "Abstract\n",
      "Chain-of-Thought (CoT) prompting can dra-\n",
      "matically improve the multi-step reasoning abil-\n",
      "ities of large language models (LLMs). CoT\n",
      "explicitly encourages the LLM to generate in-\n",
      "termediate rationales for solving a problem, by\n",
      "providing a series of reasoning steps in the\n",
      "demonstrations. Despite its success, there is\n",
      "still little understanding of what makes CoT\n",
      "prompting effective and which aspects of the\n",
      "demonstrated reasoning steps contribute to its\n",
      "performance. In this paper, we show that\n",
      "CoT reasoning is possible even with invalid\n",
      "demonstrations—prompting with invalid rea-\n",
      "soning steps can achieve over 80-90% of the\n",
      "performance obtained using CoT under various\n",
      "metrics, while still generating coherent lines\n",
      "of reasoning during inference. Further experi-\n",
      "ments show that other aspects of the rationales,\n",
      "such as being relevant to the query and correctly\n",
      "ordering the reasoning steps, are much more\n",
      "important for effective CoT reasoning. Overall,\n",
      "these findings both deepen our understanding\n",
      "of CoT prompting, and open up new questions\n",
      "regarding LLMs’ capability to learn to reason\n",
      "in context.1\n",
      "1 Introduction\n",
      "\n",
      "whether chain-of-thought can solve them. arXiv\n",
      "preprint arXiv:2210.09261 .\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\n",
      "Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\n",
      "and Denny Zhou. 2023. Self-consistency improves\n",
      "chain of thought reasoning in language models. In\n",
      "The Eleventh International Conference on Learning\n",
      "Representations .\n",
      "Albert Webson and Ellie Pavlick. 2022. Do prompt-\n",
      "based models really understand the meaning of their\n",
      "prompts? In Proceedings of the 2022 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies , pages 2300–2344, Seattle, United States.\n",
      "Association for Computational Linguistics.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\n",
      "and Denny Zhou. 2022. Chain-of-thought prompt-\n",
      "ing elicits reasoning in large language models. In\n",
      "\n",
      "dialogue in this work.\n",
      "Chain of Thought Chain-of-thought (CoT) prompting induces LLMs to generate intermediate reasoning\n",
      "steps before answering [Wei et al., 2022b]. There are two lines of research focusing on the current CoT\n",
      "prompting. One line is exploring the manually designed CoT. In the manually designed CoT, LLMs adapt\n",
      "the manually designed features and demonstration for the reasoning process [Wei et al., 2022b]. Wang et al.\n",
      "[2022] proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in\n",
      "chain-of-thought prompting. Recently, Interactive-Chain-Prompting [Pilault et al., 2023] is introduced to\n",
      "resolve the ambiguity for crosslingual conditional generation. Another line is conducting research on the\n",
      "zero-shot setting, where STaR [Zelikman et al., 2022] is introduced for the self-generation and helps the\n",
      "model to self-improve, and Automatic Reasoning and Tool-use (ART) [Paranjape et al., 2023] is a framework\n",
      "that uses frozen LLMs to automatically generate intermediate reasoning steps as a program.\n",
      "GPT-based Systems GPT [Brown et al., 2020] has shown promising performance improvements. A recent\n",
      "line of research has focused on integrating the GPT model into AI systems. HuggingGPT [Shen et al.,\n",
      "8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards Understanding Chain-of-Thought Prompting:\n",
      "An Empirical Study of What Matters\n",
      "Boshi Wang1Sewon Min2Xiang Deng1Jiaming Shen3You Wu3\n",
      "Luke Zettlemoyer2Huan Sun1\n",
      "1The Ohio State University2University of Washington3Google Research\n",
      "{wang.13930,deng.595,sun.397}@osu.edu\n",
      "{sewon,lsz}@cs.washington.edu ,{jmshen,wuyou}@google.com\n",
      "Abstract\n",
      "Chain-of-Thought (CoT) prompting can dra-\n",
      "matically improve the multi-step reasoning abil-\n",
      "ities of large language models (LLMs). CoT\n",
      "explicitly encourages the LLM to generate in-\n",
      "termediate rationales for solving a problem, by\n",
      "providing a series of reasoning steps in the\n",
      "demonstrations. Despite its success, there is\n",
      "still little understanding of what makes CoT\n",
      "prompting effective and which aspects of the\n",
      "demonstrated reasoning steps contribute to its\n",
      "performance. In this paper, we show that\n",
      "CoT reasoning is possible even with invalid\n",
      "demonstrations—prompting with invalid rea-\n",
      "soning steps can achieve over 80-90% of the\n",
      "performance obtained using CoT under various\n",
      "metrics, while still generating coherent lines\n",
      "of reasoning during inference. Further experi-\n",
      "ments show that other aspects of the rationales,\n",
      "such as being relevant to the query and correctly\n",
      "ordering the reasoning steps, are much more\n",
      "important for effective CoT reasoning. Overall,\n",
      "these findings both deepen our understanding\n",
      "of CoT prompting, and open up new questions\n",
      "regarding LLMs’ capability to learn to reason\n",
      "in context.1\n",
      "1 Introduction\n",
      "\n",
      "whether chain-of-thought can solve them. arXiv\n",
      "preprint arXiv:2210.09261 .\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\n",
      "Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\n",
      "and Denny Zhou. 2023. Self-consistency improves\n",
      "chain of thought reasoning in language models. In\n",
      "The Eleventh International Conference on Learning\n",
      "Representations .\n",
      "Albert Webson and Ellie Pavlick. 2022. Do prompt-\n",
      "based models really understand the meaning of their\n",
      "prompts? In Proceedings of the 2022 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies , pages 2300–2344, Seattle, United States.\n",
      "Association for Computational Linguistics.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\n",
      "and Denny Zhou. 2022. Chain-of-thought prompt-\n",
      "ing elicits reasoning in large language models. In\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__4c9a3644dee14218912f9ad032923e90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
